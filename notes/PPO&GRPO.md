![image](https://github.com/user-attachments/assets/436c6aeb-fcac-42ac-b763-60e4db663fa1)

## [PPO and GRPO](https://www.zhihu.com/question/12933942086/answer/116987640415)
强化学习（RL）已被证明在监督微调（SFT）阶段后能有效地进一步提高LLM的数学推理能力。在本节中，我们将介绍我们高效且有效的RL算法——组相对策略优化（GRPO）。 
Proximal Policy Optimization（PPO）是一种行为评价RL算法，广泛应用于LLM的RL微调阶段。特别是，它通过最大化以下替代目标来优化LLM：

![image](https://github.com/user-attachments/assets/c42ed470-2a3b-47bd-88db-0da91171ab0d)

其中𝜋𝜃 和 𝜋𝜃𝑜𝑙𝑑 是当前和旧的策略模型，以及𝑞, 𝑜是分别从问题数据集和旧策略𝜋𝜃𝑜𝑙𝑑中采样的问题和输出。 𝜀 是PPO中引入的一个与剪切相关的超参数，用于稳定训练。
𝐴𝑡是通过应用广义优势估计（GAE）计算的优势，基于奖励r>=t，以及一个可学习的价值函数V。因此，在PPO中，需要与策略模型一起训练价值函数，并减轻奖励模型的过度优化，
标准方法是在每个token的奖励中添加参考模型中的每个tokenKL惩罚。
![image](https://github.com/user-attachments/assets/1566fab0-c4f8-4f0c-bbfc-5e7d590d8977)
其中，哪里𝑟𝜑 是奖励模型，𝜋𝑟𝑒𝑓 是参考模型，通常是初始SFT模型，以及𝛽是KL惩罚系数.

由于PPO中使用的价值函数通常是与策略模型大小相当的另一个模型，因此它带来了大量的内存和计算负担。此外，在RL训练期间，价值函数被视为计算方差减少优势的基线。
而在LLM环境中，通常只有最后一个token被奖励模型分配奖励分数，这可能会使在每个token上准确的价值函数的训练复杂化。
为了解决这个问题，如图4所示，我们提出了组相对策略优化（GRPO），它消除了在PPO中对额外价值函数近似的需要，而是使用响应同一问题而产生的多个采样输出的平均奖励作为基线。
更具体地说，对于每个问题q,GRPO从旧策略中采样一组输出{𝑜1,𝑜2,...𝑜G} 然后通过最大化以下目标来优化策略模型：
![image](https://github.com/user-attachments/assets/16477439-2a72-4ced-ac07-173a3c36da93)

其中𝜀 and 𝛽 是超参数，以及𝐴ˆ𝑖,𝑡是仅基于每组内部输出的相对回报计算的优势，将在以下小节中详细说明。
`GRPO用来计算优势的群体相对方式与奖励模型的比较性质一致，因为奖励模型通常是在同一问题的输出之间的比较数据集上训练的。`
另请注意，GRPO不是在奖励中添加KL惩罚，而是通过将训练策略和参考策略之间的KL差异直接添加到损失中来进行正则化，从而避免使计算复杂化𝐴ˆ𝑖,𝑡.与（2）中使用的KL惩罚项不同，我们用以下无偏估计量估计KL散度，这保证是正的。


![image](https://github.com/user-attachments/assets/086562d8-ae85-4566-8463-37c2aa4fdb90)


![image](https://github.com/user-attachments/assets/95fd54b9-0c07-4954-9cfd-373396f67bde)

### 4.1.2. Outcome Supervision RL with GRPO(结果监督RL)
形式上，对于每个问题,从旧策略模型𝜋𝜃𝑜𝑙d 中采样一组输出{𝑜1,𝑜2,...𝑜G}. 然后使用奖励模型对输出进行评分，得出G，奖励r={𝑟1, 𝑟2, ... 𝑟G}.
随后，通过减去组平均值并除以组标准差来对这些奖励进行归一化。结果监督在每个输出结束时提供标准化的奖励𝑜𝑖并设置优势𝐴ˆ𝑖,𝑡 将输出中的所有token作为归一化奖励。然后通过最大化方程式（3）中定义的目标来优化策略。
𝐴ˆ𝑖,𝑡 = 𝑟𝑖 = 𝑟𝑖−mean(r)/std(r)

### 4.1.3. Process Supervision RL with GRPO
结果监督只在每个输出结束时提供奖励，这可能不足以有效地监督复杂数学任务中的策略。我们还探索了过程监督，它在每个推理步骤结束时提供奖励。

### 4.1.4. Iterative RL with GRPO
随着强化学习训练过程的推进，旧的奖励模型可能不足以监督当前的策略模型。因此，我们还探索了使用GRPO的迭代RL。
如算法1所示，在迭代GRPO中，我们根据策略模型的采样结果为奖励模型生成新的训练集，并使用包含10%历史数据的重放机制不断训练旧的奖励模型。
然后，我们将参考模型设置为策略模型，并用新的奖励模型不断训练策略模型。
<img width="1548" height="684" alt="image" src="https://github.com/user-attachments/assets/704eba8b-e80c-4e40-a68c-51f07ca7da19" />


---
### GRPO与PPO的相同点与不同点

#### **相同点**
1. **策略优化框架**  
   GRPO（Group Relative Policy Optimization）与PPO（Proximal Policy Optimization）均属于**策略梯度强化学习算法**，核心目标是通过优化策略网络参数，最大化模型在任务中的长期累积奖励。两者均直接学习策略模型（Actor），生成动作或文本响应。

2. **基于优势函数的更新**  
   两者均利用**优势函数（Advantage Function）**衡量动作的优劣，指导策略更新。优势函数反映了实际奖励与预期奖励的差异，帮助模型区分高价值与低价值动作。

3. **重要性采样机制**  
   为稳定训练，两者均采用**重要性采样**（Importance Sampling）技术，通过新旧策略的概率比值（\(\frac{\pi_\theta}{\pi_{\theta_{\text{old}}}}\)）调整梯度更新幅度，避免策略突变导致的训练崩溃。

4. **梯度裁剪或约束**  
   PPO通过**裁剪目标函数**或**KL散度惩罚**限制策略更新幅度；GRPO虽未显式裁剪，但通过**组内相对奖励归一化**间接约束更新范围，确保策略平稳优化。

5. **适用于语言模型对齐**  
   两者均被广泛应用于大型语言模型（LLM）的微调，如基于人类反馈的强化学习（RLHF），通过奖励模型（Reward Model）提供奖励信号，优化模型生成内容的质量、安全性或一致性。

#### **不同点**
| **维度**         | **PPO**                          | **GRPO**                          |
|------------------|----------------------------------|----------------------------------|
| **核心架构**     | **Actor-Critic双网络**：策略网络（Actor）生成动作，价值网络（Critic）估计状态价值，提供基线（Baseline）以降低方差。 | **去Critic化**：仅保留策略网络，通过**组内相对奖励**（Group Relative Reward）计算优势函数，无需价值网络。 |
| **优势函数计算** | 依赖价值网络估计状态价值 \(V(s)\)，优势函数为 \(A(s,a) = r + \gamma V(s') - V(s)\)（其中 \(\gamma\) 为折扣因子）。 | 对同一输入生成多个候选输出（组），用奖励模型评分后，通过**组内归一化**（减去均值、除以标准差）计算优势函数，反映相对优劣。 |
| **计算效率**     | 需同时训练Actor和Critic，计算开销较大，尤其在模型规模较大时。 | 仅需训练策略网络，减少参数和计算资源消耗，工程实现更轻量。 |
| **训练稳定性**   | 价值网络可能引入估计误差，导致优势函数偏差；KL惩罚或裁剪可缓解策略突变。 | 依赖组内样本的多样性，若采样不足或奖励模型区分度低，可能导致训练信号模糊；但去Critic化减少了误差传播路径。 |
| **适用场景**     | 通用性强，适用于机器人控制、游戏、对话生成等任务，尤其需要精确状态价值估计的场景。 | 专为大规模语言模型设计，适配多候选输出场景（如数学推理、编程任务），通过相对奖励强化专项能力。 |
| **奖励机制**     | 依赖全局奖励信号，需价值网络提供基线。 | 依赖组内相对奖励，无需全局基线，更聚焦局部对比优化。 |

#### **关键差异分析**
1. **Critic网络的存在与否**  
   PPO的Critic网络是其核心组件，通过估计状态价值提供基线，降低奖励方差；但需额外训练参数，增加计算成本。GRPO通过组内相对奖励归一化替代Critic，简化架构，适配大模型高效训练需求。

2. **优势函数的归一化方式**  
   PPO的优势函数基于全局价值估计，可能受Critic误差影响；GRPO的优势函数通过组内样本归一化，更稳定但依赖采样质量。例如，在数学推理任务中，GRPO可生成多个解题步骤，通过奖励模型评分后，直接比较步骤间的相对正确性，优化策略。

3. **训练信号来源**  
   PPO依赖全局奖励信号和Critic的基线估计；GRPO通过组内对比生成局部训练信号，更适用于需要精细对比优化的任务（如代码生成、逻辑推理）。

4. **工程实现复杂度**  
   GRPO去除了Critic网络，减少了参数规模和训练步骤，在资源受限场景（如微调70亿参数模型）中更具优势；PPO需同时维护Actor和Critic，计算成本更高，但通用性更强。
   
[PPO与GRPO的相同点与不同点](https://yiyan.baidu.com/share/88TSCcLB5E)
