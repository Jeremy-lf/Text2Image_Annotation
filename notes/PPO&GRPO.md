![image](https://github.com/user-attachments/assets/436c6aeb-fcac-42ac-b763-60e4db663fa1)

## [PPO and GRPO](https://www.zhihu.com/question/12933942086/answer/116987640415)
强化学习（RL）已被证明在监督微调（SFT）阶段后能有效地进一步提高LLM的数学推理能力。在本节中，我们将介绍我们高效且有效的RL算法——组相对策略优化（GRPO）。 
Proximal Policy Optimization（PPO）是一种行为评价RL算法，广泛应用于LLM的RL微调阶段。特别是，它通过最大化以下替代目标来优化LLM：

![image](https://github.com/user-attachments/assets/c42ed470-2a3b-47bd-88db-0da91171ab0d)

其中𝜋𝜃 和 𝜋𝜃𝑜𝑙𝑑 是当前和旧的策略模型，以及𝑞, 𝑜是分别从问题数据集和旧策略𝜋𝜃𝑜𝑙𝑑中采样的问题和输出。 𝜀 是PPO中引入的一个与剪切相关的超参数，用于稳定训练。
𝐴𝑡是通过应用广义优势估计（GAE）计算的优势，基于奖励r>=t，以及一个可学习的价值函数V。因此，在PPO中，需要与策略模型一起训练价值函数，并减轻奖励模型的过度优化，
标准方法是在每个token的奖励中添加参考模型中的每个tokenKL惩罚。
![image](https://github.com/user-attachments/assets/1566fab0-c4f8-4f0c-bbfc-5e7d590d8977)
其中，哪里𝑟𝜑 是奖励模型，𝜋𝑟𝑒𝑓 是参考模型，通常是初始SFT模型，以及𝛽是KL惩罚系数.

由于PPO中使用的价值函数通常是与策略模型大小相当的另一个模型，因此它带来了大量的内存和计算负担。此外，在RL训练期间，价值函数被视为计算方差减少优势的基线。
而在LLM环境中，通常只有最后一个token被奖励模型分配奖励分数，这可能会使在每个token上准确的价值函数的训练复杂化。
为了解决这个问题，如图4所示，我们提出了组相对策略优化（GRPO），它消除了在PPO中对额外价值函数近似的需要，而是使用响应同一问题而产生的多个采样输出的平均奖励作为基线。
更具体地说，对于每个问题q,GRPO从旧策略中采样一组输出{𝑜1,𝑜2,...𝑜G} 然后通过最大化以下目标来优化策略模型：
![image](https://github.com/user-attachments/assets/16477439-2a72-4ced-ac07-173a3c36da93)

其中𝜀 and 𝛽 是超参数，以及𝐴ˆ𝑖,𝑡是仅基于每组内部输出的相对回报计算的优势，将在以下小节中详细说明。
`GRPO用来计算优势的群体相对方式与奖励模型的比较性质一致，因为奖励模型通常是在同一问题的输出之间的比较数据集上训练的。`
另请注意，GRPO不是在奖励中添加KL惩罚，而是通过将训练策略和参考策略之间的KL差异直接添加到损失中来进行正则化，从而避免使计算复杂化𝐴ˆ𝑖,𝑡.与（2）中使用的KL惩罚项不同，我们用以下无偏估计量估计KL散度，这保证是正的。


![image](https://github.com/user-attachments/assets/086562d8-ae85-4566-8463-37c2aa4fdb90)


![image](https://github.com/user-attachments/assets/95fd54b9-0c07-4954-9cfd-373396f67bde)

### 4.1.2. Outcome Supervision RL with GRPO(结果监督RL)
形式上，对于每个问题,从旧策略模型𝜋𝜃𝑜𝑙d 中采样一组输出{𝑜1,𝑜2,...𝑜G}. 然后使用奖励模型对输出进行评分，得出G，奖励r={𝑟1, 𝑟2, ... 𝑟G}.
随后，通过减去组平均值并除以组标准差来对这些奖励进行归一化。结果监督在每个输出结束时提供标准化的奖励𝑜𝑖并设置优势𝐴ˆ𝑖,𝑡 将输出中的所有token作为归一化奖励。然后通过最大化方程式（3）中定义的目标来优化策略。
𝐴ˆ𝑖,𝑡 = 𝑟𝑖 = 𝑟𝑖−mean(r)/std(r)

### 4.1.3. Process Supervision RL with GRPO
结果监督只在每个输出结束时提供奖励，这可能不足以有效地监督复杂数学任务中的策略。我们还探索了过程监督，它在每个推理步骤结束时提供奖励。

### 4.1.4. Iterative RL with GRPO
随着强化学习训练过程的推进，旧的奖励模型可能不足以监督当前的策略模型。因此，我们还探索了使用GRPO的迭代RL。
如算法1所示，在迭代GRPO中，我们根据策略模型的采样结果为奖励模型生成新的训练集，并使用包含10%历史数据的重放机制不断训练旧的奖励模型。
然后，我们将参考模型设置为策略模型，并用新的奖励模型不断训练策略模型。
<img width="1548" height="684" alt="image" src="https://github.com/user-attachments/assets/704eba8b-e80c-4e40-a68c-51f07ca7da19" />
