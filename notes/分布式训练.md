
```
# 机器1（IP: 192.168.1.1）
MASTER_ADDR=192.168.1.1 MASTER_PORT=12355 \
python -m torch.distributed.launch \
    --nproc_per_node=4 \  # 每台机器的GPU数量
    --nnodes=2 \         # 总机器数
    --node_rank=0 \      # 当前机器的rank（0或1）
    train.py

# 机器2（IP: 192.168.1.2）
MASTER_ADDR=192.168.1.1 MASTER_PORT=12355 \
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=2 \
    --node_rank=1 \
    train.py
```

```
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def main():
    # 初始化方式一：手动设置 RANK 和 WORLD_SIZE（示例：单节点 8 GPU）
    # os.environ['RANK'] = '0'  # 当前进程的 rank（0 到 7）
    # os.environ['WORLD_SIZE'] = '1'  # 总进程数
    # os.environ['MASTER_ADDR'] = '10.96.203.76'  # 主节点地址
    # os.environ['MASTER_PORT'] = '29222'  # 主节点端口

    # 初始化分布式环境
    # dist.init_process_group(backend="nccl", init_method="env://")


    # 初始化方式二：初始化分布式环境
    dist.init_process_group(backend="nccl", init_method="env://")
    rank = dist.get_rank()
    world_size = dist.get_world_size() # 自动获取进程总数, WORLD_SIZE必须等于nnodes × nproc_per_node（如2机×4卡=8）
    local_rank = int(os.environ["LOCAL_RANK"])  # torch.distributed.launch自动设置，自动获取单机内进程ID(0-nproc_per_node-1)
    total_gpus = torch.cuda.device_count() # 获取单机内的总GPU数量
    # 绑定GPU
    torch.cuda.set_device(local_rank)
    print(f"Rank {rank} (Local {local_rank}) uses GPU {torch.cuda.current_device()}")

    # 模型和数据加载...
    model = torch.nn.Linear(10, 5).cuda(local_rank)
    model = DDP(model, device_ids=[local_rank])

    # 训练逻辑...

if __name__ == "__main__":
    main()

```

`python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_port 29511 demo_dd.py`

`torchrun --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=10.96.203.76 --master_port=29222 train.py`

---

在PyTorch分布式训练中，`local_rank`和`rank`（通常指`global_rank`）是两个关键但易混淆的概念，它们分别描述进程在**单机内**和**全局**的编号。以下是详细对比和实际应用场景的说明：

---

### **一、核心区别**
| **属性**       | **`local_rank`**                     | **`rank`（或`global_rank`）**         |
|----------------|--------------------------------------|---------------------------------------|
| **作用范围**   | 单台机器内的进程编号（0到`nproc_per_node-1`） | 所有机器的全局唯一进程编号（0到`WORLD_SIZE-1`） |
| **典型值**     | 单机4卡时：`0, 1, 2, 3`             | 2机8卡时：机器0的进程为`0-3`，机器1的进程为`4-7` |
| **用途**       | 绑定当前进程的GPU、区分单机内数据分片 | 协调全局通信（如AllReduce）、区分不同机器的进程 |
| **获取方式**   | 通过`LOCAL_RANK`环境变量或`torch.distributed.get_rank(group=None)`（需指定本地组） | 通过`dist.get_rank()`或`RANK`环境变量 |

---

### **二、为什么需要区分两者？**
#### **1. GPU绑定**
- **问题**：多机训练时，每台机器可能有多个GPU，需确保每个进程绑定到正确的本地GPU。
- **解决方案**：
  ```python
  local_rank = int(os.environ["LOCAL_RANK"])  # 从环境变量获取
  torch.cuda.set_device(local_rank)           # 绑定GPU
  model = MyModel().cuda(local_rank)          # 将模型放到对应GPU
  ```
  - 若直接用`rank`绑定GPU，可能导致机器1的进程`rank=4`错误绑定到机器0的GPU。

#### **2. 数据分片**
- **问题**：全局数据需均匀分配到所有进程，但单机内可能需进一步细分。
- **示例**：
  ```python
  # 全局数据分片（基于rank）
  sampler = DistributedSampler(dataset, num_replicas=dist.get_world_size(), rank=dist.get_rank())
  
  # 若单机内需要额外分片（如多模型并行），可结合local_rank
  if dist.get_rank() == 0:  # 仅主进程操作
      local_subset = dataset[local_rank::4]  # 假设单机4进程，每进程取1/4数据
  ```

#### **3. 日志与调试**
- **问题**：全局日志可能因多机器输出而混乱，需区分本地和全局信息。
- **实践**：
  ```python
  print(f"[Global Rank {dist.get_rank()}] [Local Rank {local_rank}] Starting training...")
  ```
  - 输出示例：`[Global Rank 3] [Local Rank 1] Starting training...`（表示机器1的第2个进程）。

---

### **三、代码示例：完整使用场景**
```python
import os
import torch
import torch.distributed as dist
from torch.utils.data import DataLoader, DistributedSampler

def main():
    # 初始化分布式环境（自动读取RANK/WORLD_SIZE/MASTER_ADDR等）
    dist.init_process_group(backend="nccl", init_method="env://")
    
    # 获取全局和本地rank
    global_rank = dist.get_rank()
    local_rank = int(os.environ["LOCAL_RANK"])  # 需通过torchrun或启动脚本设置
    world_size = dist.get_world_size()
    
    # 绑定GPU
    torch.cuda.set_device(local_rank)
    print(f"Global Rank {global_rank}, Local Rank {local_rank}, GPU {torch.cuda.current_device()}")
    
    # 模型定义（放到对应GPU）
    model = torch.nn.Linear(10, 2).cuda(local_rank)
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])
    
    # 数据加载（全局分片）
    dataset = torch.utils.data.TensorDataset(torch.randn(1000, 10), torch.randint(0, 2, (1000,)))
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=global_rank, shuffle=True)
    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)
    
    # 训练循环
    for epoch in range(2):
        sampler.set_epoch(epoch)  # 确保每次epoch数据顺序不同
        for data, target in dataloader:
            data, target = data.cuda(local_rank), target.cuda(local_rank)
            # 训练步骤...

if __name__ == "__main__":
    main()
```

---

### **四、启动命令对比**
#### **1. 单机多卡（无需显式设置LOCAL_RANK）**
```bash
torchrun --nproc_per_node=4 train.py
```
- PyTorch会自动设置`LOCAL_RANK`为`0,1,2,3`，`RANK`为全局编号（单机时与`LOCAL_RANK`相同）。

#### **2. 多机训练（需显式传递LOCAL_RANK）**
```bash
# 机器0（node_rank=0）
MASTER_ADDR=10.96.203.76 MASTER_PORT=29222 \
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 train.py

# 机器1（node_rank=1）
MASTER_ADDR=10.96.203.76 MASTER_PORT=29222 \
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=1 train.py
```
- `LOCAL_RANK`由`torchrun`自动设置，`RANK`计算方式为：  
  `RANK = node_rank * nproc_per_node + local_rank`  
  （如机器1的第2个进程：`RANK = 1*4 + 1 = 5`）。

---

### **五、常见错误**
1. **混淆`local_rank`和`rank`绑定GPU**：
   - ❌ 错误：`torch.cuda.set_device(dist.get_rank())`  
     （可能导致不同机器的进程绑定到同一GPU）
   - ✅ 正确：`torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))`

2. **未设置`LOCAL_RANK`环境变量**：
   - 若直接运行脚本而非通过`torchrun`启动，需手动设置：
     ```bash
     LOCAL_RANK=0 RANK=0 WORLD_SIZE=1 python train.py
     ```

3. **数据分片错误**：
   - 使用`DistributedSampler`时，必须传入`rank=dist.get_rank()`而非`local_rank`，否则数据无法全局均匀分配。

---

### **总结**
- **`local_rank`**：单机内进程标识，用于GPU绑定和本地资源分配。
- **`rank`（全局）**：跨机器进程标识，用于全局通信和数据分片。
- **最佳实践**：始终通过`torchrun`或`init_method="env://"`自动管理环境变量，避免手动设置错误。
