## 位置编码（Position Embedding, PE）

在大模型（尤其是Transformer架构）中，位置编码（Positional Encoding）用于捕捉序列中元素的顺序信息，弥补自注意力机制无法直接感知位置关系的缺陷。以下是常用的位置编码类型及其特点：

### 1. **绝对位置编码（Absolute Positional Encoding）**
   - **正弦/余弦位置编码（Sinusoidal PE）**  
     - **原理**：使用不同频率的正弦和余弦函数生成位置信息，公式为：
       $\[
       PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right), \quad PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
       \]$ 其中pos是位置，i是维度索引，d_model是维度。
     - **特点**：
       - 固定编码，不依赖训练数据。
       - 支持外推（extrapolation），可处理比训练时更长的序列。
       - 相对位置信息通过线性组合隐式学习。
     - **应用**：原始Transformer（如BERT、GPT的早期版本）。
```python
import torch
import math

class SinusoidalPositionalEncoding(torch.nn.Module):
    def __init__(self, dim_model, max_len=512):
        super().__init__()
        pe = torch.zeros(max_len, dim_model)
        position = torch.arange(max_len).unsqueeze(1).float() # e^log(a)=a
        div_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0) / dim_model))
        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数位用sin
        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数位用cos
        self.register_buffer('pe', pe.unsqueeze(0))  # shape: [1, max_len, dim_model]

    def forward(self, x):
        # x: [batch_size, seq_len, dim_model]
        return x + self.pe[:, :x.size(1)]  # 截取到当前序列长度
```
   - **可学习绝对位置编码（Learned Absolute PE）**  
     - **原理**：将位置编码作为可训练参数矩阵，与输入嵌入相加。
     - **特点**：
       - 灵活性强，能自动学习适合任务的编码方式。
       - 但外推能力差，序列长度超过训练范围时性能下降。
     - **应用**：GPT-2、GPT-3等。

```python
class LearnedPositionalEncoding(torch.nn.Module):
    def __init__(self, dim_model, max_len=512):
        super().__init__()
        self.pe = torch.nn.Parameter(torch.zeros(max_len, dim_model))  # 可学习参数
        torch.nn.init.normal_(self.pe, mean=0.0, std=0.02)  # 初始化

    def forward(self, x):
        # x: [batch_size, seq_len, dim_model]
        return x + self.pe[:x.size(1)]  # 截取到当前序列长度
```


---

### 2. **相对位置编码（Relative Positional Encoding）**
   - **原理**：直接建模元素间的相对距离，而非绝对位置。
   - **典型方法**：
     - **Transformer-XL的相对位置编码**：  
       引入可学习的相对位置矩阵，计算注意力时区分相对距离的偏置项。
     - **DeBERTa的相对位置编码**：  
       将相对位置拆分为距离和方向（如“前/后”），通过独立参数建模。
     - **T5的相对位置偏置（Relative Position Bias）**：  
       在注意力分数中添加与相对距离相关的可学习偏置项。
   - **特点**：
     - 更符合语言任务的局部依赖性。
     - 外推能力有限，但可通过截断或分段处理长序列。
   - **应用**：Transformer-XL、DeBERTa、T5等。

```python
class RelativePositionalEncoding(torch.nn.Module):
    def __init__(self, dim_model, max_len=512):
        super().__init__()
        self.max_len = max_len
        # 相对位置偏置矩阵 (2K+1是相对位置范围，例如[-K, K])
        self.rel_pos_bias = torch.nn.Parameter(torch.zeros(2 * max_len - 1, dim_model))
        torch.nn.init.normal_(self.rel_pos_bias, mean=0.0, std=0.02)

    def forward(self, x, attention_scores):
        # x: [batch_size, seq_len, dim_model]
        # attention_scores: [batch_size, num_heads, seq_len, seq_len]
        seq_len = x.size(1)
        # 生成相对位置索引 (例如seq_len=3时，相对位置为[0,1,2]和[0,-1,-2])
        rel_pos = torch.arange(seq_len).unsqueeze(0) - torch.arange(seq_len).unsqueeze(1)
        rel_pos = rel_pos.clamp(-self.max_len + 1, self.max_len - 1) + self.max_len - 1  # 映射到[0, 2K]
        # 获取相对位置偏置并reshape
        bias = self.rel_pos_bias[rel_pos].unsqueeze(0)  # [1, seq_len, seq_len, dim_model]
        # 将偏置投影到注意力头维度（简化版：直接加到注意力分数）
        bias = bias.mean(dim=-1)  # 假设单头，实际需适配多头
        return attention_scores + bias
```
---


### 3. **旋转位置编码（Rotary Positional Encoding, RoPE）**
在Transformer模型中，自注意力机制本身并不包含位置信息，因此需要显式地注入位置编码来帮助模型理解序列中元素的顺序。传统的位置编码方法，如绝对位置编码（如BERT中的正弦编码）和相对位置编码（如T5中的偏置项），都存在一定的局限性。<font color=Red>**绝对位置编码难以建模相对位置关系，而相对位置编码计算复杂度高，且外推性差（即在处理比训练时更长的序列时性能会下降）**</font>。为了解决这些问题，RoPE被提出作为一种新的位置编码方法。

RoPE以绝对位置编码形式实现的相对位置编码，它通过将一个向量旋转某个角度，为其赋予位置信息。具体操作是对attention中的q, k进行旋转变换，使其自带相对位置信息，然后用更新的q,k向量计算attention，得到的内积就会引入相对位置信息。旋转矩阵是通过复数矩阵旋转实现的
   - **原理**：将位置信息嵌入到旋转矩阵中，通过复数运算实现位置感知。公式为：
     <div align=center> <img src="https://github.com/user-attachments/assets/716f7cba-f889-4b90-91c0-1f4ad8bfc919" width = 40% > </div> 
     其中 $( \theta_i = 10000^{-2i/d} )$， m是位置。

   - **特点**：
     - 显式编码相对位置，且相对位置的计算与绝对位置无关。
     - 支持外推，长序列性能稳定。
     - 计算效率高，适合现代硬件。
   - **应用**：LLaMA、GPT-NeoX、PaLM等主流大模型。

```python
class RotaryPositionalEncoding(torch.nn.Module):
    def __init__(self, dim_model, max_len=512):
        super().__init__()
        # 生成旋转矩阵的参数
        position = torch.arange(max_len).float().unsqueeze(1)
        div_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0) / dim_model))
        self.register_buffer('cos_pos', torch.cos(position * div_term))  # [max_len, dim_model//2]
        self.register_buffer('sin_pos', torch.sin(position * div_term))  # [max_len, dim_model//2]

    def forward(self, x):
        # x: [batch_size, seq_len, dim_model]
        seq_len = x.size(1)
        x_rope = x.view(*x.shape[:-1], -1, 2)  # [..., dim_model//2, 2]
        cos_pos = self.cos_pos[:seq_len].unsqueeze(0).unsqueeze(-2)  # [1, seq_len, 1, dim_model//2]
        sin_pos = self.sin_pos[:seq_len].unsqueeze(0).unsqueeze(-2)
        # 旋转操作：x_rope * [cos, -sin; sin, cos]
        x_rope_rotated = torch.stack(
            [x_rope[..., 0] * cos_pos - x_rope[..., 1] * sin_pos,
             x_rope[..., 0] * sin_pos + x_rope[..., 1] * cos_pos],
            dim=-1
        ).flatten(-2)  # [..., dim_model]
        return x_rope_rotated
```

---


### 4. **其他变体**
   - **ALiBi（Attention with Linear Biases）**  
     - 在注意力分数中添加与距离成反比的线性偏置，替代传统位置编码。
     - 特点：简单高效，外推能力强，但可能损失部分位置精度。
     - 应用：GPT-NeoX、Pythia等。

```python
class ALiBiPositionalBias(torch.nn.Module):
    def __init__(self, num_heads, max_len=512):
        super().__init__()
        self.num_heads = num_heads
        # 生成ALiBi的斜率（固定或可学习）
        self.slopes = torch.nn.Parameter(torch.tensor(1 / (2 ** (8 - torch.arange(num_heads).float() / (num_heads - 1)))))
        self.register_buffer('bias_matrix', torch.tril(torch.ones(max_len, max_len)))

    def forward(self, attention_scores):
        # attention_scores: [batch_size, num_heads, seq_len, seq_len]
        seq_len = attention_scores.size(-1)
        bias = self.bias_matrix[:seq_len, :seq_len].unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]
        # 缩放偏置：m * slopes * distance
        distance_matrix = torch.arange(seq_len).unsqueeze(0) - torch.arange(seq_len).unsqueeze(1)  # [seq_len, seq_len]
        alibi_bias = distance_matrix.unsqueeze(0).unsqueeze(1) * self.slopes.view(1, -1, 1, 1)  # [1, num_heads, seq_len, seq_len]
        alibi_bias = alibi_bias * bias  # 只保留下三角（因果掩码）
        return attention_scores + alibi_bias
```

   - **3D位置编码**  
     - 结合空间信息（如图像中的像素坐标），通过额外通道或可学习矩阵编码。
     - 应用：Vision Transformer（ViT）、VideoBERT等。

### **对比总结**
| **类型**         | **优点**                          | **缺点**                          | **典型模型**       |
|------------------|-----------------------------------|-----------------------------------|--------------------|
| 正弦/余弦PE      | 固定编码，支持外推                | 相对位置信息隐式学习              | 原始Transformer    |
| 可学习绝对PE      | 灵活性强                          | 外推能力差                        | GPT-2              |
| 相对位置编码      | 显式建模相对距离                  | 实现复杂，外推有限                | Transformer-XL      |
| RoPE             | 高效外推，相对位置显式计算        | 实现稍复杂                        | LLaMA, GPT-NeoX    |
| ALiBi            | 简单高效，外推能力强              | 可能损失位置精度                  | GPT-NeoX           |

### **选择建议**
- **短序列任务**：可学习绝对PE或正弦PE足够。
- **长序列/外推需求**：优先选择RoPE或ALiBi。
- **需要显式相对位置**：Transformer-XL或DeBERTa的方案。
- **计算效率敏感场景**：RoPE或ALiBi更优。

位置编码的选择需结合任务需求、序列长度和模型架构综合考量，现代大模型（如LLaMA系列）普遍采用RoPE以平衡性能与效率。

### 使用示例
```python
# 示例：在Transformer中集成位置编码
dim_model = 512
seq_len = 10
batch_size = 2

# 1. 正弦位置编码
sin_pe = SinusoidalPositionalEncoding(dim_model)
x = torch.randn(batch_size, seq_len, dim_model)
x_with_pe = sin_pe(x)

# 2. RoPE
rope = RotaryPositionalEncoding(dim_model)
x_rotated = rope(x)

# 3. ALiBi（需在注意力计算后应用）
alibi = ALiBiPositionalBias(num_heads=8)
attention_scores = torch.randn(batch_size, 8, seq_len, seq_len)
attention_with_alibi = alibi(attention_scores)
```


参考：[位置编码](https://2048.csdn.net/680b30fee47cbf761b60f790.html?dp_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6NjI3NTU4LCJleHAiOjE3NTI0MTQyMTQsImlhdCI6MTc1MTgwOTQxNCwidXNlcm5hbWUiOiJKZXJlbXlfbGYifQ.mX-YQF0G5LbD9x5UGaHTO7YtQdxu4drpGBWiVodKBjk&spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Eactivity-3-146075127-blog-143246495.235%5Ev43%5Epc_blog_bottom_relevance_base5&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Eactivity-3-146075127-blog-143246495.235%5Ev43%5Epc_blog_bottom_relevance_base5&utm_relevant_index=5)


---

**RoPE（Rotary Positional Embedding）是一种用于Transformer模型中注入相对位置信息的方法**，它通过旋转矩阵将位置信息融入查询（Query）和键（Key）向量的计算中，使得模型能够自然地感知序列中元素之间的相对位置关系，同时支持任意长度的上下文建模。以下是对RoPE的详细解释：

### 一、RoPE的提出背景

在Transformer模型中，自注意力机制本身并不包含位置信息，因此需要显式地注入位置编码来帮助模型理解序列中元素的顺序。传统的位置编码方法，如绝对位置编码（如BERT中的正弦编码）和相对位置编码（如T5中的偏置项），都存在一定的局限性。绝对位置编码难以建模相对位置关系，而相对位置编码计算复杂度高，且外推性差（即在处理比训练时更长的序列时性能会下降）。为了解决这些问题，RoPE被提出作为一种新的位置编码方法。

### 二、RoPE的核心原理

RoPE的核心思想是通过旋转矩阵将位置信息编码到查询和键向量的计算中。具体来说，对于每个位置$i$的查询向量$q_i$和位置$j$的键向量$k_j$，RoPE将它们分别通过旋转矩阵$R_i$和$R_j$进行变换，得到新的向量$q_i'$和$k_j'$。然后，计算这两个新向量的内积，即$\text{Attention}(q_i, k_j) = (q_i')^T \cdot (k_j')$。这个内积结果不仅包含了原始查询和键向量的信息，还隐含了它们之间的相对位置关系。

旋转矩阵$R_i$是基于位置$i$和维度索引$d$定义的，它通过复数旋转的方式实现。对于每个维度$d$，定义一个旋转角度$\theta_d = 10000^{-\frac{2d}{D}}$，其中$D$是向量的总维度。然后，对位置$i$定义旋转向量$R_i$，它是一个由正弦和余弦函数组成的矩阵，作用于查询和键向量，实现位置感知的注意力计算。

### 三、RoPE的优势与特点

1. **相对位置编码**：RoPE使得注意力计算能够自然地引入位置依赖，且内积结果只依赖于位置差，与绝对位置无关。这有助于模型更好地理解序列中元素的相对位置关系。
2. **长期衰减**：RoPE具有远程衰减的特性，即距离越远的位置对当前位置的影响越小。这符合语言建模的直觉，因为相邻的词项通常比远离的词项更相关。
3. **支持长序列建模**：由于RoPE支持任意长度的上下文建模，因此它特别适合处理长序列数据。这使得模型在处理长文本、长视频等序列数据时具有更好的性能。
4. **计算效率高**：RoPE的实现相对简单，且计算效率高。它不需要额外的参数或复杂的计算过程，因此可以轻松地集成到现有的Transformer模型中。

### 四、RoPE的实现方式

RoPE的实现通常涉及以下几个步骤：

1. **定义旋转矩阵**：根据位置索引和维度索引计算旋转角度，并构造旋转矩阵。
2. **应用旋转矩阵**：将旋转矩阵应用于查询和键向量，得到新的向量表示。
3. **计算注意力分数**：使用新的查询和键向量计算注意力分数，并应用softmax函数得到注意力权重。
4. **聚合信息**：根据注意力权重聚合值向量的信息，得到最终的输出表示。

### 五、RoPE在大模型中的应用

RoPE已成为现代大语言模型的标准配置之一。例如，Qwen3模型使用了RoPE来替代传统的可学习位置嵌入，并结合ABF（Adaptive BaseFrequency）扩展了位置频率范围，从而支持高达128K tokens的上下文长度。此外，ChatGLM系列模型（如ChatGLM-6B、ChatGLM3）也采用了RoPE编码机制，使得模型在中文理解与生成任务中具备良好的上下文建模能力，尤其在医学、法律、金融等长文本场景中表现出色。
