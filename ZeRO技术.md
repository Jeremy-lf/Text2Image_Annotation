**ZeRO（Zero Redundancy Optimizer）** 是微软研究院提出的一种**显存优化技术**，专为大规模深度学习模型训练设计。它的核心思想是通过**分区（Partitioning）**和**并行化**消除传统数据并行（Data Parallelism）中的冗余内存占用，从而支持训练数十亿甚至万亿参数的模型。

---

## **1. 传统数据并行的痛点**
在标准的数据并行（如PyTorch的`DistributedDataParallel`）中：
- **每个GPU存储完整的模型参数、梯度和优化器状态**（如Adam的动量、方差）。
- **显存占用高**：例如，训练一个10B参数的模型，使用Adam优化器时，优化器状态可能占用 **40GB显存/GPU**（FP16参数 + FP32优化器状态）。
- **扩展性差**：随着模型规模增大，单GPU显存成为瓶颈，无法通过增加GPU数量线性扩展。

---

## **2. ZeRO的核心思想**
ZeRO通过**分阶段消除冗余**，将模型参数、梯度和优化器状态**分区存储**到不同GPU上，同时保证计算正确性。它分为三个阶段（Stage 1/2/3），**逐步减少显存占用**：

### **Stage 1: 优化器状态分区（Optimizer State Partitioning）**
- **问题**：传统数据并行中，每个GPU存储完整的优化器状态（如Adam的动量）。
- **ZeRO-1**：将优化器状态**平均分配到所有GPU**，每个GPU仅存储部分参数的优化器状态。
- **效果**：优化器状态显存占用减少到 **1/N**（N为GPU数量）。
- **通信开销**：梯度聚合时需要额外通信（但可通过重叠计算掩盖）。

### **Stage 2: 梯度分区（Gradient Partitioning）**
- **问题**：ZeRO-1仍需每个GPU存储完整梯度（用于反向传播后的参数更新）。
- **ZeRO-2**：在反向传播时，**梯度计算后立即分区存储**，不再保留完整梯度。
- **效果**：梯度显存占用减少到 **1/N**，优化器状态+梯度总显存占用减少到 **2/N**。
- **关键技术**：
  - **梯度分片存储**：每个GPU仅保留部分梯度。
  - **参数更新时动态聚合**：需要时通过`All-Gather`收集完整梯度（但通常与优化器状态分区结合，避免全局聚合）。

### **Stage 3: 参数分区（Parameter Partitioning）**
- **问题**：ZeRO-2仍需每个GPU存储完整模型参数（用于前向传播）。
- **ZeRO-3**：将模型参数**完全分区**，每个GPU仅存储部分参数。
- **效果**：
  - 参数显存占用减少到 **1/N**。
  - **总显存占用减少到 ~1/N**（参数+梯度+优化器状态）。
- **关键技术**：
  - **参数分片加载**：前向传播时，通过`All-Gather`动态收集所需参数。
  - **通信与计算重叠**：隐藏参数聚合的通信开销。

---

## **3. ZeRO与其他技术的对比**
| 技术               | 显存优化点                     | 通信开销       | 适用场景               |
|--------------------|-------------------------------|----------------|------------------------|
| **数据并行**       | 无优化（完整复制）             | 低             | 小模型（<1B参数）      |
| **模型并行**       | 参数分片（层间/层内）         | 高（跨节点）   | 超大规模模型（>100B） |
| **ZeRO-1/2**       | 优化器状态+梯度分区            | 中等           | 大模型（1B~100B）     |
| **ZeRO-3**         | 参数+梯度+优化器状态全分区     | 高（可重叠）   | 极限规模模型（>100B）  |
| **ZeRO-Infinity**  | 结合CPU/NVMe卸载               | 极高（异构）   | 内存不足的极端场景     |

---

## **4. ZeRO的实际效果**
- **显存节省**：以10B参数模型为例：
  - 传统数据并行：~40GB/GPU（Adam优化器）。
  - ZeRO-3：~4GB/GPU（参数+梯度+优化器状态全分区）。
- **扩展性**：支持训练**万亿参数模型**（如GPT-3级模型）。
- **性能**：通过通信与计算重叠，训练速度接近数据并行（在GPU数量较多时可能更慢，但显存效率更高）。

---

## **5. 常见实现框架**
- **DeepSpeed**：微软官方实现，支持ZeRO-1/2/3和ZeRO-Infinity。
- **FairScale**（已弃用）：Meta早期实现，后被PyTorch集成。
- **Megatron-DeepSpeed**：结合Megatron（模型并行）和DeepSpeed（ZeRO）的混合方案。

---

## **6. 典型配置示例（DeepSpeed）**
```json
{
  "zero_optimization": {
    "stage": 3,                  # 启用ZeRO-3
    "overlap_comm": true,         # 通信与计算重叠
    "contiguous_gradients": true, # 梯度连续存储
    "reduce_bucket_size": 1e8    # 梯度聚合的桶大小
  }
}
```

---

## **总结**
ZeRO通过**分区存储**和**动态通信**解决了大规模模型训练的显存瓶颈，是当前训练千亿/万亿参数模型的核心技术之一。它的三个阶段（Stage 1/2/3）逐步优化显存占用，而ZeRO-3是当前最常用的配置，能显著降低单GPU显存需求，同时保持较高的训练效率。


## 问题：训练跟推理一个10B的模型需要多少参数量？
训练需同时存储‌模型参数、梯度、优化器状态及中间激活值‌，具体显存占用如下：

1.全精度（FP32）+ Adam优化器‌：
* 参数：40GB
* 梯度：40GB
* 优化器状态（一阶/二阶动量）：80GB
‌* 小计：160GB‌（未含激活值）

‌2.半精度（BF16/FP16）+ Adam‌：
* 参数+梯度：40GB
* 优化器状态（通常保留FP32）：80GB
‌* 小计：120GB‌

‌3.激活值占用‌：
* 与批次大小、序列长度相关，10B模型可达数十GB

‌4.综合建议‌：
* 全精度训练：‌需≥192GB显存‌（通常需多卡并行，如4×A100 80GB）
* 半精度训练：‌需≥144GB显存‌（如2×A100 80GB）
